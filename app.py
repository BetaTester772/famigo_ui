import cv2
import mediapipe as mp
from facenet_pytorch import InceptionResnetV1
from PIL import Image
import torchvision.transforms as transforms
import os
from enum import Enum
import torch
import numpy as np
import sounddevice as sd
from collections import deque
import time
import soundfile as sf
import uuid

import ssl

ssl._create_default_https_context = ssl._create_unverified_context


# =========================
# VAD Recorder
# =========================

class VADRecorder:
    def __init__(self):
        # Load model
        self.model, self.utils = torch.hub.load(
                repo_or_dir='snakers4/silero-vad',
                model='silero_vad',
                # force_reload=True,
                trust_repo=True
        )
        self.vad_iterator = self.utils[3](self.model)  # VADIterator is the 4th element

        # Settings
        self.SAMPLE_RATE = 16000
        self.BUFFER_SIZE = self.SAMPLE_RATE * 60  # 1 minute buffer
        self.THRESHOLD = 0.5  # TODO: fit to environment
        self.MIN_DURATION = 0.5
        self.MARGIN = 1
        self.SILENCE_TIME = 0.6

        # State
        self.reset_state()

    def reset_state(self):
        self.audio_buffer = deque(maxlen=self.BUFFER_SIZE)
        self.is_speaking = False
        self.speech_start_sample = None
        self.sample_counter = 0
        self.silence_counter = 0
        self.ema_speech_prob = 0
        self.saved_filename = None

    def _save_audio_segment(self, start_sample, end_sample):
        audio_array = np.array(list(self.audio_buffer), dtype=np.int16)
        start = max(0, start_sample - int(self.MARGIN * self.SAMPLE_RATE))
        end = min(len(audio_array), end_sample + int(self.MARGIN * self.SAMPLE_RATE))
        segment = audio_array[start:end]

        if len(segment) / self.SAMPLE_RATE < self.MIN_DURATION:
            print(f"Segment too short, skipping save.")
            return

        filename = f"speech_{time.strftime('%Y%m%d_%H%M%S')}.wav"
        sf.write(filename, segment, self.SAMPLE_RATE)
        print(f"Audio saved: {filename}")
        self.saved_filename = filename

    def _callback(self, indata, frames, time_info, status):
        if status:
            print(status)

        if self.saved_filename:  # Stop if we already have a file
            return

        audio_int16 = (indata * 32768).astype(np.int16).flatten()
        self.audio_buffer.extend(audio_int16)

        if len(audio_int16) < 512:
            return

        audio_tensor = torch.from_numpy(audio_int16).float()
        speech_prob = self.vad_iterator.model(audio_tensor, self.SAMPLE_RATE).item()
        self.ema_speech_prob = 0.9 * self.ema_speech_prob + 0.1 * speech_prob

        if self.ema_speech_prob > self.THRESHOLD:
            if not self.is_speaking:
                self.is_speaking = True
                self.speech_start_sample = self.sample_counter
            self.silence_counter = 0
        else:
            if self.is_speaking:
                self.silence_counter += frames / self.SAMPLE_RATE
                if self.silence_counter >= self.SILENCE_TIME:
                    self.is_speaking = False
                    speech_end_sample = self.sample_counter
                    duration = (speech_end_sample - self.speech_start_sample) / self.SAMPLE_RATE
                    if duration >= self.MIN_DURATION:
                        self._save_audio_segment(self.speech_start_sample, speech_end_sample)

        self.sample_counter += frames

    def record(self, timeout=10):
        self.reset_state()
        stream = sd.InputStream(callback=self._callback, channels=1, samplerate=self.SAMPLE_RATE, blocksize=512)
        with stream:
            print("Listening for speech...")
            start_time = time.time()
            while time.time() - start_time < timeout:
                if self.saved_filename:
                    break
                sd.sleep(100)

        print("Finished listening.")
        return self.saved_filename


# For use in other scripts
def listen_and_record_speech(timeout=10):
    """
    Creates a VADRecorder instance and records one speech segment.
    Returns the filename or None.
    """
    recorder = VADRecorder()
    filename = recorder.record(timeout=timeout)
    return filename


# =========================
# State Definition
# =========================

class State(Enum):
    IDLE = 0
    USER_CHECK = 1
    ENROLL = 2
    WELCOME = 3
    ASR = 4
    BYE = 5


# =========================
# Globals & Flags
# =========================

FACE_DETECTED = False
USER_EXIST = False
ENROLL_SUCCESS = False
VAD = False
BYE_EXIST = False
TIMER_EXPIRED = False  # WELCOME, BYE state's timer
ASR_TEXT = ""

# Shared between states
sh_face_crop = None
sh_bbox = None
sh_embedding = None
sh_current_user = None
sh_audio_file = None
sh_message = "Initializing..."
sh_color = (255, 255, 0)
sh_timer_end = 0
sh_prev_unkonw = None

# =========================
# Utils
# =========================

DB_PATH = "faces_db.npy"
SIM_THRESHOLD = 0.65


def load_db():
    if os.path.exists(DB_PATH):
        data = np.load(DB_PATH, allow_pickle=True).item()
        return data["name_list"], data["group_list"], data["embeddings"]
    else:
        return [], [], np.empty((0, 512))


def save_db(name_list, group_list, embeddings):
    np.save(DB_PATH, {"name_list": name_list, "group_list": group_list, "embeddings": embeddings})


def find_match(embedding, name_list, embeddings):
    if len(embeddings) == 0:
        return None, 0
    sims = [np.dot(embedding, emb) / (np.linalg.norm(embedding) * np.linalg.norm(emb)) for emb in embeddings]
    max_idx = np.argmax(sims)
    if sims[max_idx] >= SIM_THRESHOLD:
        return name_list[max_idx], sims[max_idx]
    else:
        return None, sims[max_idx]


# Face detection & bbox
def update_face_detection():
    global FACE_DETECTED, sh_face_crop, sh_bbox, sh_frame

    image = sh_frame.copy()
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = face_detection.process(rgb_image)

    if results.detections and len(results.detections) == 1:
        FACE_DETECTED = True
        detection = results.detections[0]
        bboxC = detection.location_data.relative_bounding_box
        ih, iw, _ = image.shape
        x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)
        x, y = max(0, x), max(0, y)
        sh_bbox = (x, y, w, h)
        sh_face_crop = image[y:y + h, x:x + w]
        if sh_face_crop.size == 0:
            FACE_DETECTED = False
    else:
        FACE_DETECTED = False
        sh_bbox = None
        sh_face_crop = None

    return results


# =========================
# ASR (Whisper)
# =========================

import whisper

whisper_model = whisper.load_model("large-v3")


def asr_from_wav(file_path: str) -> str:
    print(f"./{file_path}",
          os.path.exists(f"./{file_path}"))
    result = whisper_model.transcribe(f"./{file_path}")
    print(result)
    return result['text']


# =========================
# State Action Functions
# =========================

def enter_idle():
    global sh_message, sh_color
    results = update_face_detection()
    if not FACE_DETECTED:
        if results.detections and len(results.detections) > 1:
            sh_message = f"{len(results.detections)} faces detected. Only one please."
            sh_color = (0, 0, 255)
        else:
            sh_message = "Waiting for user..."
            sh_color = (255, 255, 0)


def enter_user_check():
    global USER_EXIST, sh_embedding, sh_current_user, sh_message, sh_color

    update_face_detection()
    if sh_face_crop is None:
        return

    face_pil = Image.fromarray(cv2.cvtColor(sh_face_crop, cv2.COLOR_BGR2RGB))
    face_tensor = preprocess(face_pil).unsqueeze(0)
    with torch.no_grad():
        embedding = resnet(face_tensor)[0].cpu().numpy()
    sh_embedding = embedding / np.linalg.norm(embedding)

    match_name, sim = find_match(sh_embedding, name_list, embeddings)
    if match_name:
        USER_EXIST = True
        sh_current_user = match_name
        sh_message = f"Identifying... {match_name} ({sim:.2f})"
        sh_color = (0, 255, 0)
    else:
        USER_EXIST = False
        sh_message = "Unknown user. Use the right panel to enroll."
        sh_color = (0, 255, 255)


def enter_enroll(key=None):
    # key kept for signature compatibility; not used
    global ENROLL_SUCCESS, sh_message, sh_color

    results = update_face_detection()

    if not FACE_DETECTED:
        if results.detections and len(results.detections) > 1:
            sh_message = f"{len(results.detections)} faces detected. Only one please."
            sh_color = (0, 0, 255)
        else:
            sh_message = "Please show your face to the camera for enrollment."
            sh_color = (255, 255, 0)
    else:
        sh_message = "Unknown user. Use the right panel to enroll."
        sh_color = (0, 255, 255)


def enter_welcome():
    global VAD, sh_audio_file, TIMER_EXPIRED, sh_message, sh_color

    update_face_detection()

    TIMER_EXPIRED = False
    VAD = False
    sh_message = f"Hi, {sh_current_user}!"
    sh_color = (0, 255, 0)

    if time.time() > sh_timer_end:
        TIMER_EXPIRED = True
        # cv2.destroyAllWindows()  # GUI ì°½ì„ ì“°ì§€ ì•Šìœ¼ë¯€ë¡œ ìƒëµ

        sh_audio_file = listen_and_record_speech(timeout=5)
        if sh_audio_file:
            VAD = True


def enter_asr():
    global BYE_EXIST, ASR_TEXT

    update_face_detection()

    text = asr_from_wav(sh_audio_file)
    ASR_TEXT = text
    text = "".join(text.split())
    if "ì˜ê°€" in text or "bye" in text.lower():
        BYE_EXIST = True
    else:
        BYE_EXIST = False


def enter_bye():
    global TIMER_EXPIRED, sh_message, sh_color

    update_face_detection()

    TIMER_EXPIRED = False
    sh_message = f"Bye, {sh_current_user}!"
    sh_color = (255, 0, 255)

    if time.time() > sh_timer_end:
        TIMER_EXPIRED = True


# =========================
# Transitions & Dispatcher
# =========================

def state_transition(current_state: State) -> State:
    global sh_prev_unkonw, sh_embedding, name_list, group_list, embeddings

    if current_state == State.IDLE:
        return State.USER_CHECK if FACE_DETECTED else State.IDLE

    elif current_state == State.USER_CHECK:
        return State.WELCOME if USER_EXIST else State.ENROLL

    elif current_state == State.ENROLL:
        # Stay in ENROLL until success; go IDLE only if face lost
        if ENROLL_SUCCESS:
            print("[Enroll Success] Reloading DB...")
            name_list, group_list, embeddings = load_db()
            return State.WELCOME
        sh_prev_unkonw = sh_embedding
        return State.IDLE if not FACE_DETECTED else State.ENROLL

    elif current_state == State.WELCOME:
        if TIMER_EXPIRED:
            return State.ASR if VAD else State.IDLE
        return State.WELCOME

    elif current_state == State.ASR:
        return State.BYE if BYE_EXIST else State.IDLE

    elif current_state == State.BYE:
        return State.IDLE if TIMER_EXPIRED else State.BYE

    return current_state


def call_state_fn(state: State, key):
    if state == State.IDLE:
        enter_idle()
    elif state == State.USER_CHECK:
        enter_user_check()
    elif state == State.ENROLL:
        enter_enroll(key)
    elif state == State.WELCOME:
        enter_welcome()
    elif state == State.ASR:
        enter_asr()
    elif state == State.BYE:
        enter_bye()


# =========================
# Model Init
# =========================

print("Loading models...")
resnet = InceptionResnetV1(pretrained='vggface2').eval()
mp_face_detection = mp.solutions.face_detection
face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)
preprocess = transforms.Compose([
        transforms.Resize((160, 160)), transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
name_list, group_list, embeddings = load_db()
print("Models loaded.")

# =========================
# Streamlit UI & Main Loop
# =========================

import streamlit as st

st.set_page_config(page_title="Face Kiosk", layout="wide")
st.title("ğŸ‘¤ Face Kiosk with State UI")

# Layout
col_video, col_ui = st.columns([3, 2], vertical_alignment="top")

# Camera / Options
with col_video:
    st.subheader("ğŸ“· Camera")
    cam_index = st.number_input("Camera index", min_value=0, max_value=10, value=0, step=1)
    width = st.slider("Frame width", 320, 1920, 640, step=10)
    run = st.toggle("Run camera", value=False)
    frame_slot = st.empty()

# UI placeholders
with col_ui:
    st.subheader("ğŸ§­ State Panel")
    state_badge = st.empty()
    message_slot = st.empty()
    enroll_slot = st.empty()
    welcome_slot = st.empty()
    asr_slot = st.empty()
    bye_slot = st.empty()
    audio_slot = st.empty()
    debug_slot = st.expander("Debug", expanded=False)

# Keep only camera handle in session_state
if "cap" not in st.session_state:
    st.session_state.cap = None

if "enroll_form_key" not in st.session_state:
    st.session_state.enroll_form_key = None


def open_camera(index: int, target_w: int):
    cap = cv2.VideoCapture(index)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_w)
    return cap


# ENROLL UI lifecycle flags
ENROLL_UI_BUILT = False
enroll_face_ph = None

# Initial state
state = State.IDLE
st.caption("Starting state machine...")


# ENROLL submit helper (uses Streamlit; defined after importing st)
def ui_enroll_submit(new_name: str, new_group: str):
    global ENROLL_SUCCESS, name_list, group_list, embeddings, sh_current_user

    if not new_name or new_name.strip() == "":
        st.warning("ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
        return
    if sh_embedding is None or len(sh_embedding) == 0:
        st.error("ì–¼êµ´ ì„ë² ë”©ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ì— ì–¼êµ´ì„ ë˜‘ë°”ë¡œ ë¹„ì¶°ì£¼ì„¸ìš”.")
        return
    if any(n == new_name for n in name_list):
        st.warning("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì´ë¦„ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”.")
        return

    name_list.append(new_name)
    group_list.append(new_group)

    if embeddings.size:
        embeddings = np.vstack([embeddings, sh_embedding])
    else:
        embeddings = np.array([sh_embedding])

    save_db(name_list, group_list, embeddings)

    sh_current_user = new_name
    ENROLL_SUCCESS = True
    USER_EXIST = True

    st.success(f"ë“±ë¡ ì™„ë£Œ: {new_name} ({new_group if new_group else 'group ë¯¸ì§€ì •'})")
    print("[DB Updated] ", name_list, group_list, embeddings.shape)


if run:
    # Open camera once
    if st.session_state.cap is None or not st.session_state.cap.isOpened():
        st.session_state.cap = open_camera(int(cam_index), int(width))
        if not st.session_state.cap.isOpened():
            st.error("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ë°”ê¾¸ê±°ë‚˜ ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œí•´ë³´ì„¸ìš”.")
            st.stop()


    # UI helper: render state panel
    def render_state_panel(current_state: State):
        global ENROLL_UI_BUILT, enroll_face_ph

        # Badge
        state_badge.markdown(f"**Current State:** :blue[{current_state.name}]")

        # --- ì¤‘ìš”: í˜„ì¬ ìƒíƒœê°€ ENROLLì¼ ë•ŒëŠ” enroll_slotì„ ë¹„ìš°ì§€ ì•ŠëŠ”ë‹¤!
        if current_state != State.ENROLL:
            enroll_slot.empty()  # ENROLLì„ ë²—ì–´ë‚˜ëŠ” ìˆœê°„ì—ë§Œ ë¹„ì›€

        # ë‹¤ë¥¸ ìƒíƒœ ìŠ¬ë¡¯ë“¤ì€ ë§¤ í”„ë ˆì„ ì´ˆê¸°í™” ê°€ëŠ¥
        if current_state != State.WELCOME:
            welcome_slot.empty()
        if current_state != State.ASR:
            asr_slot.empty()
        if current_state != State.BYE:
            bye_slot.empty()

        # Message
        with message_slot.container():
            st.markdown(f"**Message:** {sh_message}")

        # ENROLL UI (form created once, with unique key)
        if current_state == State.ENROLL:
            # ENROLLì— ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ê³ ìœ í•œ í¼ í‚¤ë¥¼ 1íšŒ ìƒì„±
            if st.session_state.enroll_form_key is None:
                st.session_state.enroll_form_key = f"form_enroll_{uuid.uuid4().hex}"
                # í¼ì´ ìƒˆë¡œ ë§Œë“¤ì–´ì§ˆ ê²ƒì´ë¯€ë¡œ, í‘œì‹œìš© face placeholderë„ ìƒˆë¡œ ë°›ìŒ
                ENROLL_UI_BUILT = False
                enroll_face_ph = None

            if not ENROLL_UI_BUILT:
                ENROLL_UI_BUILT = True
                with enroll_slot.container():
                    st.info("ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ìš©ìì…ë‹ˆë‹¤. ì•„ë˜ í¼ìœ¼ë¡œ ë“±ë¡ì„ ì§„í–‰í•˜ì„¸ìš”.")
                    enroll_face_ph = st.empty()

                    form_key = st.session_state.enroll_form_key
                    # ì…ë ¥ ìœ„ì ¯ í‚¤ë„ í¼ í‚¤ì— ì¢…ì†ì‹œì¼œ ì¤‘ë³µ ë°©ì§€
                    with st.form(key=form_key, clear_on_submit=False):
                        new_name = st.text_input("ì´ë¦„", key=f"{form_key}_name")
                        new_group = st.text_input("ê·¸ë£¹(ì„ íƒ)", key=f"{form_key}_group")
                        submitted = st.form_submit_button("ë“±ë¡í•˜ê¸°", use_container_width=True)
                    if submitted:
                        ui_enroll_submit(new_name, new_group)
                        # ë“±ë¡ ì§í›„ì—ëŠ” ë°”ë¡œ ì „ì´ë˜ë¯€ë¡œ(UIìƒ) í¼ í‚¤ ì •ë¦¬(ì•ˆì „)
                        st.session_state.enroll_form_key = None

            # ì–¼êµ´ ë¯¸ë¦¬ë³´ê¸°ëŠ” ê³„ì† ê°±ì‹ 
            if enroll_face_ph is not None:
                if sh_face_crop is not None and sh_face_crop.size != 0:
                    face_rgb = cv2.cvtColor(sh_face_crop, cv2.COLOR_BGR2RGB)
                    enroll_face_ph.image(face_rgb, caption="ë“±ë¡í•  ì–¼êµ´", use_container_width=True)
                else:
                    enroll_face_ph.warning("ì–¼êµ´ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë¥¼ í–¥í•´ í•œ ëª…ë§Œ ë¹„ì¶°ì£¼ì„¸ìš”.")

        else:
            # ENROLLì´ ì•„ë‹ˆë©´ í¼ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ë‹¤ìŒ ENROLL ë•Œ ìƒˆ í‚¤/í¼ ìƒì„±)
            if ENROLL_UI_BUILT:
                ENROLL_UI_BUILT = False
                enroll_face_ph = None
            enroll_slot.empty()
            # í˜¹ì‹œ ë‚¨ì•„ìˆëŠ” í¼ í‚¤ê°€ ìˆìœ¼ë©´ ì œê±°
            st.session_state.enroll_form_key = None

        # WELCOME UI
        if current_state == State.WELCOME:
            with welcome_slot.container():
                st.success(f"Hi, **{sh_current_user}**! ì ì‹œ í›„ ìŒì„± ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                remain = max(0.0, sh_timer_end - time.time())
                pct = min(max(1.0 - (remain / 2.0), 0.0), 1.0)
                st.progress(pct, text="Greeting...")

        # ASR UI
        if current_state == State.ASR:
            with asr_slot.container():
                st.info("ìŒì„± ì¸ì‹ ê²°ê³¼")
                if sh_audio_file:
                    audio_slot.audio(sh_audio_file)
                    st.write("ë…¹ìŒ íŒŒì¼ ì¬ìƒ ê°€ëŠ¥")
                if ASR_TEXT:
                    st.write(f"**ì¸ì‹ëœ í…ìŠ¤íŠ¸:** {ASR_TEXT}")
                st.write(f"**BYE detected:** {'Yes' if BYE_EXIST else 'No'}")

        # BYE UI
        if current_state == State.BYE:
            with bye_slot.container():
                st.warning(f"Bye, **{sh_current_user}**!")
                remain = max(0.0, sh_timer_end - time.time())
                pct = min(max(1.0 - (remain / 2.0), 0.0), 1.0)
                st.progress(pct, text="Ending...")


    # Main loop
    while run:
        success, sh_frame = st.session_state.cap.read()
        if not success:
            st.error("í”„ë ˆì„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            break

        # Key (kept for compatibility; not used for enroll)
        key = cv2.waitKey(1) & 0xFF

        # Call & transition
        call_state_fn(state, key)
        new_state = state_transition(state)

        if new_state != state:
            print(f"State Change: {state.name} -> {new_state.name}")

            # ENROLL 'ì§„ì…' ì‹œ í•œ ë²ˆë§Œ ì´ˆê¸°í™”
            if new_state == State.ENROLL:
                ENROLL_SUCCESS = False
                USER_EXIST = False
                # ì´ì „ í¼ í‚¤ê°€ ë‚¨ì•„ìˆìœ¼ë©´ ì§€ì›€ (ì•ˆì „)
                st.session_state.enroll_form_key = None

            # ENROLL 'ì´íƒˆ' ì‹œ í¼ í‚¤ ì œê±° (ì¤‘ë³µ ë°©ì§€)
            if state == State.ENROLL and new_state != State.ENROLL:
                st.session_state.enroll_form_key = None

            state = new_state
            if state == State.WELCOME:
                sh_timer_end = time.time() + 2.0
            elif state == State.BYE:
                sh_timer_end = time.time() + 2.0

        # Update state panel
        render_state_panel(state)

        # â”€â”€ ì˜ìƒ ê·¸ë¦¬ê¸°/í‘œì‹œ (ASRì´ ì•„ë‹ ë•Œë§Œ í‘œì‹œ)
        if state != State.ASR and sh_frame is not None:
            display_frame = sh_frame.copy()

            if sh_bbox:
                x, y, w, h = sh_bbox
                cv2.rectangle(display_frame, (x, y), (x + w, y + h), sh_color, 2)
                cv2.putText(display_frame, sh_message, (x, y - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, sh_color, 2)
            else:
                cv2.putText(display_frame, sh_message, (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, sh_color, 2)

            frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            h0, w0, _ = frame_rgb.shape
            new_h = int(h0 * (width / w0))
            frame_rgb = cv2.resize(frame_rgb, (int(width), new_h))
            frame_slot.image(frame_rgb, channels="RGB", caption="Live", use_container_width=True)
        else:
            # ASR ë‹¨ê³„: ì¹´ë©”ë¼ëŠ” ê³„ì† ì½ì§€ë§Œ, UIì—ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ
            frame_slot.empty()

        # Debug info
        with debug_slot:
            st.write({
                    "FACE_DETECTED" : FACE_DETECTED,
                    "USER_EXIST"    : USER_EXIST,
                    "ENROLL_SUCCESS": ENROLL_SUCCESS,
                    "VAD"           : VAD,
                    "BYE_EXIST"     : BYE_EXIST,
                    "TIMER_EXPIRED" : TIMER_EXPIRED,
                    "current_user"  : sh_current_user,
                    "audio_file"    : sh_audio_file
            })

        time.sleep(0.01)
        run = st.session_state.get("_toggle_run", True)

    # Cleanup
    if st.session_state.cap is not None:
        st.session_state.cap.release()
        st.session_state.cap = None
        frame_slot.empty()
        st.info("ì¹´ë©”ë¼ë¥¼ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
